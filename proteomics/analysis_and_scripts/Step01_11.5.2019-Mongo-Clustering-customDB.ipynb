{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002170085906982422\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#--------------------FUNCTIONS FOR LARGE PROTEIN FASTA ASSEMBLY USING MONGODB--------------------\n",
    "\n",
    "#Extract proteinIDs from DTASelect Proteins for mongoDB protein fasta assembly\n",
    "#input: DTAselect Protein file\n",
    "#output: proteinID list\n",
    "def extractProteinID(DTA_select_file):\n",
    "    proteinlist1 = []\n",
    "    with open(DTA_select_file, 'r') as h:\n",
    "        file1 = h.readlines()\n",
    "    for line1 in file1:\n",
    "        line2 = line1.split('\\t')\n",
    "        if (line2[0] == \"Unique\"):\n",
    "            index1 = file1.index(line1)\n",
    "            break\n",
    "    for line1 in range((index1+1), len(file1)):\n",
    "        line2 = file1[line1].split('\\t')\n",
    "        if (line2[0] != \"\") and (line2[0] != '*') and (line2[1] != \"Proteins\"):\n",
    "            proteinlist1.append(line2[0])\n",
    "        elif (line2[1] == \"Proteins\"):\n",
    "            break\n",
    "    return proteinlist1\n",
    "\n",
    "#search mongodatabase for protein sequences\n",
    "#input: proteinID list and name of output fasta file\n",
    "#output: fasta file containing proteinIDs and protein sequences\n",
    "def findProtSeq(plist1, output_file):\n",
    "    client = MongoClient(\"localhost\", 27017)\n",
    "    db = client['ProtDB_compil1501']\n",
    "    outputfile1 = open(output_file, 'w')\n",
    "    for item1 in plist1:\n",
    "        protein1 = db.ProtDB_compil1501.find_one({ \"b\" : item1 })\n",
    "        outputfile1.write(\">\" + protein1['d'] + \"\\n\")\n",
    "        outputfile1.write(protein1['s'] + \"\\n\")\n",
    "    outputfile1.close()\n",
    "\n",
    "#-----------------FUNCTIONS FOR ASSEMBLING CLUSTER-SPECTRAL COUNT TABLE FOR DESEQ2-----------------\n",
    "\n",
    "#create list of unique vs non-unique peptides; \n",
    "#input is DTASelect Peptide file\n",
    "#output is printed result, no return\n",
    "def determineUniqueness(file_to_open):\n",
    "    unique = []\n",
    "    nonunique = []\n",
    "    with open(file_to_open, 'r') as f:\n",
    "        file1 = f.readlines()  \n",
    "    for line1 in range(1, len(file1)):\n",
    "        line2 = file1[line1].split('\\t')\n",
    "        if '*' in line2[13]:\n",
    "            unique.append(line2[12])\n",
    "        else:\n",
    "            nonunique.append(line2[12]) \n",
    "    print(\"Unique peptides:\", len(set(unique)))\n",
    "    print(\"Non-unique peptides:\", len(set(nonunique)))\n",
    "\n",
    "#create count of all CD-HIT clusterIDs\n",
    "#input is CD-HIT cluster file\n",
    "#output is integer count of clusters\n",
    "def createClusterIndex(cluster_file):\n",
    "    with open(cluster_file, 'r') as g:\n",
    "        file1 = g.readlines()\n",
    "    clustercount = 0\n",
    "    for line1 in file1:\n",
    "        if line1[0] == \">\":\n",
    "            clustercount += 1\n",
    "    return clustercount\n",
    "\n",
    "#create dictionary linking proteinIDs (keys) to CD-HIT clusterIDs (values)\n",
    "#input is CD-HIT cluster file\n",
    "#output is ProteinID-cdhitID dictionary\n",
    "def createProteinClusterDict(cluster_file):\n",
    "    with open(cluster_file, 'r') as g:\n",
    "        file1 = g.readlines()\n",
    "    clusterID = \"\"\n",
    "    proteindict1 = {}\n",
    "    for line1 in file1:\n",
    "        line2 = line1.split()\n",
    "        if \">\" in line2[0][0]:\n",
    "            clusterID = line2[1]\n",
    "        else:\n",
    "            proteindict1[line2[2][1:-3]] = clusterID\n",
    "    return proteindict1\n",
    "\n",
    "#create file linking proteinIDs to peptides in DTAselect file \n",
    "#first input is DTASelect Protein file\n",
    "#first output is file (dump.txt) where clusterIDs(non-indented) associate with peptides (indented)\n",
    "#create dictionary linking peptides (keys; str) and clusterIDs (values); clusterIDs = list of non-redundant integers\n",
    "#second input is protein-clusterID dictionary\n",
    "#second output is peptide-ClusterID dictionary\n",
    "def createPeptideClusterIDDict(DTA_select_file, proteindict1):\n",
    "    #----------PART 1----------\n",
    "    with open(DTA_select_file, 'r') as h:\n",
    "        file1 = h.readlines()\n",
    "    for line1 in file1:\n",
    "        line2 = line1.split('\\t')\n",
    "        if (line2[0] == \"Unique\"):\n",
    "            index1 = file1.index(line1)\n",
    "            break\n",
    "    outputfile1 = open(\"dump.txt\", 'w')\n",
    "    for line1 in range((index1+1), len(file1)):\n",
    "        line2 = file1[line1].split('\\t')\n",
    "        if (line2[0] != \"\") and (line2[0] != '*') and (line2[1] != \"Proteins\"):\n",
    "            #Note this Try/Except line was added to take care of contaminant proteins in non-microbiome searches\n",
    "            try:\n",
    "                outputfile1.write(proteindict1[line2[0]] + \"\\n\")\n",
    "            except:\n",
    "                pass\n",
    "        elif ((line2[0] == \"\") or (line2[0] == '*')) and (line2[1] != \"Proteins\"):\n",
    "            #Note splice at line2[14] added to take care of non-microbiome searches\n",
    "            outputfile1.write(\"\\t\" + line2[14][1:-1] + \"\\n\")\n",
    "        else:\n",
    "            break\n",
    "    outputfile1.close()\n",
    "    #----------PART 2----------\n",
    "    with open(\"dump.txt\", 'r') as i:\n",
    "        file2 = i.readlines()\n",
    "    clusterlist1 = []\n",
    "    peptidedict1 = {}\n",
    "    newcluster = False\n",
    "    for line1 in file2:\n",
    "        line2 = line1.split('\\t')\n",
    "        if (line2[0] != ''):\n",
    "            if newcluster == True:\n",
    "                clusterlist1 = []\n",
    "                newcluster = False\n",
    "            clusterlist1.append(int(line2[0].replace('\\n', '')))\n",
    "        elif (line2[0] == ''):\n",
    "            peptidedict1[line2[1].replace('\\n', '')] = clusterlist1\n",
    "            newcluster = True\n",
    "        else:\n",
    "            break\n",
    "    for key1 in peptidedict1.keys():\n",
    "        peptidedict1[key1] = list(set(peptidedict1[key1]))\n",
    "    return peptidedict1\n",
    "\n",
    "#create dictionary linking peptides (keys; str) to spec counts (values; int)\n",
    "#input is DTASelect Peptide file\n",
    "#output is peptide-SC dictionary\n",
    "def createPeptideSCDict(file_to_open):\n",
    "    with open(file_to_open, 'r') as j:\n",
    "        file1 = j.readlines()\n",
    "    speccountdict1 = {}\n",
    "    for line1 in range(1, len(file1)):\n",
    "        line2 = file1[line1].split('\\t')\n",
    "        if line2[12] in speccountdict1:\n",
    "            speccountdict1[line2[12][1:-1].replace('\\n', '')] += int(line2[11])\n",
    "        else:\n",
    "            speccountdict1[line2[12][1:-1].replace('\\n', '')] = int(line2[11])\n",
    "    return speccountdict1\n",
    "\n",
    "#create dictionary linking clusterIDs (keys; int) to spec counts (values; int)\n",
    "#peptides mapping to >1 cluster are not counted, empty clusters are not filled in with 0\n",
    "#input is peptide-SC dictionary and peptide-ClusterID dictionary\n",
    "#output is clusterID-SC dictionary\n",
    "def createClusterIDSCDict(SCdict1, clustdict1):\n",
    "    clusterspeccountdict1 = {}\n",
    "    tempclusternumber1 = 0\n",
    "    multiclustercount = 0\n",
    "    singleclustercount = 0\n",
    "    for key1 in clustdict1.keys():\n",
    "        if len(clustdict1[key1]) == 1:\n",
    "            if clustdict1[key1][0] in clusterspeccountdict1:\n",
    "                clusterspeccountdict1[clustdict1[key1][0]] += SCdict1[key1]\n",
    "            else:\n",
    "                clusterspeccountdict1[clustdict1[key1][0]] = SCdict1[key1]\n",
    "            singleclustercount += 1\n",
    "        else:\n",
    "            #for item1 in clustdict1[key1]:\n",
    "            #    clusterspeccountdict1[item1] = 0\n",
    "            multiclustercount += 1\n",
    "    print(\"Single Cluster Assignments:\", singleclustercount)\n",
    "    print(\"Multi Cluster Assignments:\", multiclustercount)\n",
    "    return clusterspeccountdict1\n",
    "\n",
    "#input: cluster-spectral count dictionary, total number of clusters\n",
    "#output: cluster-spectral count dictionary with all cluster keys present\n",
    "def missingValues(clusterSCDict, clustercount):\n",
    "    for item1 in range(0, clustercount):\n",
    "        if item1 not in clusterSCDict.keys():\n",
    "            clusterSCDict[item1] = 0\n",
    "    return clusterSCDict\n",
    "\n",
    "#processes all lower functions for single Compil2 Search\n",
    "#input is cdhit cluster file, DTAselect protein file, and DTAselect peptide file\n",
    "#output is cluster-spectral count dictionary with 0 values filled in; cluster# (keys; int) vs SC (values; int)\n",
    "def processAll(cdhit_file, DTAselect_protein_file, DTAselect_peptide_file):\n",
    "    var1 = createProteinClusterDict(cdhit_file)\n",
    "    var2 = createPeptideClusterIDDict(DTAselect_protein_file, var1)\n",
    "    var3 = createPeptideSCDict(DTAselect_peptide_file)\n",
    "    var4 = createClusterIDSCDict(var3, var2)\n",
    "    var5 = createClusterIndex(cdhit_file)\n",
    "    var6 = missingValues(var4, var5)\n",
    "    return var6\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4076.746216058731\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#-----------Extract ProteinIDs-----------\n",
    "\n",
    "list1 = extractProteinID(\"MB-DTASelect/1-MB.txt\")\n",
    "list2 = extractProteinID(\"MB-DTASelect/2-MB.txt\")\n",
    "list3 = extractProteinID(\"MB-DTASelect/3-MB.txt\")\n",
    "list4 = extractProteinID(\"MB-DTASelect/4-MB.txt\")\n",
    "list5 = extractProteinID(\"MB-DTASelect/5-MB.txt\")\n",
    "list6 = extractProteinID(\"MB-DTASelect/6-MB.txt\")\n",
    "list7 = extractProteinID(\"MB-DTASelect/7-MB.txt\")\n",
    "list8 = extractProteinID(\"MB-DTASelect/8-MB.txt\")\n",
    "list9 = extractProteinID(\"MB-DTASelect/9-MB.txt\")\n",
    "list10 = extractProteinID(\"MB-DTASelect/10-MB.txt\")\n",
    "list11 = extractProteinID(\"MB-DTASelect/11-MB.txt\")\n",
    "list12 = extractProteinID(\"MB-DTASelect/12-MB.txt\")\n",
    "list13 = extractProteinID(\"MB-DTASelect/13-MB.txt\")\n",
    "list14 = extractProteinID(\"MB-DTASelect/14-MB.txt\")\n",
    "list15 = extractProteinID(\"MB-DTASelect/15-MB.txt\")\n",
    "list16 = extractProteinID(\"MB-DTASelect/16-MB.txt\")\n",
    "list17 = extractProteinID(\"MB-DTASelect/17-MB.txt\")\n",
    "\n",
    "#------Create Master ProteinID list------\n",
    "\n",
    "masterset1 = set(list1).union(set(list2)).union(set(list3)).union(set(list4)).union(set(list5)).union(set(list6)).union(set(list7)).union(set(list8)).union(set(list9)).union(set(list10)).union(set(list11)).union(set(list12)).union(set(list13)).union(set(list14)).union(set(list15)).union(set(list16)).union(set(list17))\n",
    "masterlist1 = list(masterset1)\n",
    "\n",
    "#------Create Master Protein Fasta-------\n",
    "\n",
    "findProtSeq(masterlist1, \"chat-fastadump1.fasta\")\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Cluster Assignments: 6702\n",
      "Multi Cluster Assignments: 798\n",
      "Single Cluster Assignments: 5445\n",
      "Multi Cluster Assignments: 653\n",
      "Single Cluster Assignments: 6504\n",
      "Multi Cluster Assignments: 696\n",
      "Single Cluster Assignments: 6106\n",
      "Multi Cluster Assignments: 729\n",
      "Single Cluster Assignments: 5688\n",
      "Multi Cluster Assignments: 765\n",
      "Single Cluster Assignments: 6289\n",
      "Multi Cluster Assignments: 729\n",
      "Single Cluster Assignments: 7073\n",
      "Multi Cluster Assignments: 775\n",
      "Single Cluster Assignments: 3879\n",
      "Multi Cluster Assignments: 556\n",
      "Single Cluster Assignments: 4591\n",
      "Multi Cluster Assignments: 690\n",
      "Single Cluster Assignments: 4875\n",
      "Multi Cluster Assignments: 753\n",
      "Single Cluster Assignments: 5232\n",
      "Multi Cluster Assignments: 661\n",
      "Single Cluster Assignments: 4363\n",
      "Multi Cluster Assignments: 578\n",
      "Single Cluster Assignments: 5170\n",
      "Multi Cluster Assignments: 714\n",
      "Single Cluster Assignments: 4485\n",
      "Multi Cluster Assignments: 615\n",
      "68.09545683860779\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#----------Process All datasets----------\n",
    "\n",
    "\"\"\"\n",
    "c1 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/1-DTAref.txt\", \"customDB-Peptides/1-Peptides.txt\")\n",
    "c2 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/2-DTAref.txt\", \"customDB-Peptides/2-Peptides.txt\")\n",
    "c3 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/3-DTAref.txt\", \"customDB-Peptides/3-Peptides.txt\")\n",
    "c4 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/4-DTAref.txt\", \"customDB-Peptides/4-Peptides.txt\")\n",
    "c5 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/5-DTAref.txt\", \"customDB-Peptides/5-Peptides.txt\")\n",
    "c6 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/6-DTAref.txt\", \"customDB-Peptides/6-Peptides.txt\")\n",
    "c7 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/7-DTAref.txt\", \"customDB-Peptides/7-Peptides.txt\")\n",
    "c8 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/8-DTAref.txt\", \"customDB-Peptides/8-Peptides.txt\")\n",
    "c9 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/9-DTAref.txt\", \"customDB-Peptides/9-Peptides.txt\")\n",
    "c10 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/10-DTAref.txt\", \"customDB-Peptides/10-Peptides.txt\")\n",
    "c11 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/11-DTAref.txt\", \"customDB-Peptides/11-Peptides.txt\")\n",
    "c12 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/12-DTAref.txt\", \"customDB-Peptides/12-Peptides.txt\")\n",
    "c13 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/13-DTAref.txt\", \"customDB-Peptides/13-Peptides.txt\")\n",
    "c14 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/14-DTAref.txt\", \"customDB-Peptides/14-Peptides.txt\")\n",
    "c15 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/15-DTAref.txt\", \"customDB-Peptides/15-Peptides.txt\")\n",
    "c16 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/16-DTAref.txt\", \"customDB-Peptides/16-Peptides.txt\")\n",
    "c17 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/17-DTAref.txt\", \"customDB-Peptides/17-Peptides.txt\")\n",
    "\"\"\"\n",
    "\n",
    "c18 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/18-DTAref.txt\", \"customDB-Peptides/18-Peptides.txt\")\n",
    "c19 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/19-DTAref.txt\", \"customDB-Peptides/19-Peptides.txt\")\n",
    "c20 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/20-DTAref.txt\", \"customDB-Peptides/20-Peptides.txt\")\n",
    "c21 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/21-DTAref.txt\", \"customDB-Peptides/21-Peptides.txt\")\n",
    "c22 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/22-DTAref.txt\", \"customDB-Peptides/22-Peptides.txt\")\n",
    "c23 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/23-DTAref.txt\", \"customDB-Peptides/23-Peptides.txt\")\n",
    "c24 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/24-DTAref.txt\", \"customDB-Peptides/24-Peptides.txt\")\n",
    "c25 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/25-DTAref.txt\", \"customDB-Peptides/25-Peptides.txt\")\n",
    "c26 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/26-DTAref.txt\", \"customDB-Peptides/26-Peptides.txt\")\n",
    "c27 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/27-DTAref.txt\", \"customDB-Peptides/27-Peptides.txt\")\n",
    "c28 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/28-DTAref.txt\", \"customDB-Peptides/28-Peptides.txt\")\n",
    "c29 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/29-DTAref.txt\", \"customDB-Peptides/29-Peptides.txt\")\n",
    "c30 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/30-DTAref.txt\", \"customDB-Peptides/30-Peptides.txt\")\n",
    "c31 = processAll(\"CDHIT/bby-ref-95.clstr\", \"customDB-DTASelect/31-DTAref.txt\", \"customDB-Peptides/31-Peptides.txt\")\n",
    "\n",
    "\n",
    "#----------Combine All datasets----------\n",
    "\n",
    "masterdict1 = {}\n",
    "dummylist1 = []\n",
    "\"\"\"\n",
    "for key1 in sorted(c1.keys()):\n",
    "    dummylist1.append(c1[key1])\n",
    "    dummylist1.append(c2[key1])\n",
    "    dummylist1.append(c3[key1])\n",
    "    dummylist1.append(c4[key1])\n",
    "    dummylist1.append(c5[key1])\n",
    "    dummylist1.append(c6[key1])\n",
    "    dummylist1.append(c7[key1])\n",
    "    dummylist1.append(c8[key1])\n",
    "    dummylist1.append(c9[key1])\n",
    "    dummylist1.append(c10[key1])\n",
    "    dummylist1.append(c11[key1])\n",
    "    dummylist1.append(c12[key1])\n",
    "    dummylist1.append(c13[key1])\n",
    "    dummylist1.append(c14[key1])\n",
    "    dummylist1.append(c15[key1])\n",
    "    dummylist1.append(c16[key1])\n",
    "    dummylist1.append(c17[key1])\n",
    "    masterdict1[key1] = dummylist1\n",
    "    dummylist1 = []\n",
    "\"\"\"   \n",
    "for key1 in sorted(c18.keys()):\n",
    "    dummylist1.append(c18[key1])\n",
    "    dummylist1.append(c19[key1])\n",
    "    dummylist1.append(c20[key1])\n",
    "    dummylist1.append(c21[key1])\n",
    "    dummylist1.append(c22[key1])\n",
    "    dummylist1.append(c23[key1])\n",
    "    dummylist1.append(c24[key1])\n",
    "    dummylist1.append(c25[key1])\n",
    "    dummylist1.append(c26[key1])\n",
    "    dummylist1.append(c27[key1])\n",
    "    dummylist1.append(c28[key1])\n",
    "    dummylist1.append(c29[key1])\n",
    "    dummylist1.append(c30[key1])\n",
    "    dummylist1.append(c31[key1])\n",
    "    masterdict1[key1] = dummylist1\n",
    "    dummylist1 = []\n",
    "\n",
    "#--------------Output Data--------------\n",
    "\n",
    "outputfile1 = open(\"customDB-THdump95.txt\", 'w')\n",
    "tempvar1 = \"\"\n",
    "tempvar2 = \"\"\n",
    "for key1 in sorted(masterdict1.keys()):\n",
    "    #if (sum(masterdict1[key1]) != 0):\n",
    "        tempvar2 = str(masterdict1[key1]).replace('[', \"\").replace(']', \"\").replace(\", \", ',')\n",
    "        tempvar1 = \"Cluster\" + str(key1) + ',' + tempvar2 + '\\n'\n",
    "        outputfile1.write(tempvar1)\n",
    "outputfile1.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
